{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMOBtYwi7/k7RXuHKE4cEPC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MoronSlayer/Deep-Learning-Projects/blob/learner/BERT_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "source: https://coaxsoft.com/blog/building-bert-with-pytorch-from-scratch"
      ],
      "metadata": {
        "id": "pOXf5twJbNxX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lv6dIqntbIoc",
        "outputId": "9220759c-804e-49d2-cfe9-33b1b5f7e937"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (6.1.2)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.64.1)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2022.6.15)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle\n"
      ],
      "metadata": {
        "id": "J-9Rf3r8cWIC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! cp kaggle.json ~/.kaggle/\n"
      ],
      "metadata": {
        "id": "X3s9KU1vcX1W"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! chmod 600 ~/.kaggle/kaggle.json\n"
      ],
      "metadata": {
        "id": "ifdVhln-cbXh"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d lakshmi25npathi/imdb-dataset-of-50k-movie-reviews"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVR7-s_wdzWr",
        "outputId": "0f602009-f589-41de-defd-2bc61eeb5ce4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading imdb-dataset-of-50k-movie-reviews.zip to /content\n",
            " 35% 9.00M/25.7M [00:00<00:00, 34.6MB/s]\n",
            "100% 25.7M/25.7M [00:00<00:00, 81.9MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip /content/imdb-dataset-of-50k-movie-reviews.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PeDmlczlciCz",
        "outputId": "50479dab-a0fa-46c1-ad95-80047b09d7f7"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/imdb-dataset-of-50k-movie-reviews.zip\n",
            "  inflating: IMDB Dataset.csv        \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd \n",
        "Dataset = pd.read_csv('/content/IMDB Dataset.csv')"
      ],
      "metadata": {
        "id": "31kiZY6dem0t"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class IMDBBertDataset(Dataset):\n",
        "    # Define Special tokens as attributes of class\n",
        "    CLS = '[CLS]'\n",
        "    PAD = '[PAD]'\n",
        "    SEP = '[SEP]'\n",
        "    MASK = '[MASK]'\n",
        "    UNK = '[UNK]'\n",
        "\n",
        "    MASK_PERCENTAGE = 0.15  # How much words to mask\n",
        "\n",
        "    MASKED_INDICES_COLUMN = 'masked_indices'\n",
        "    TARGET_COLUMN = 'indices'\n",
        "    NSP_TARGET_COLUMN = 'is_next'\n",
        "    TOKEN_MASK_COLUMN = 'token_mask'\n",
        "\n",
        "    OPTIMAL_LENGTH_PERCENTILE = 70\n",
        "\n",
        "    def __init__(self, path, ds_from=None, ds_to=None, should_include_text=False):\n",
        "        self.ds: pd.Series = pd.read_csv(path)['review']\n",
        "\n",
        "        if ds_from is not None or ds_to is not None:\n",
        "            self.ds = self.ds[ds_from:ds_to]\n",
        "\n",
        "        self.tokenizer = get_tokenizer('basic_english')\n",
        "        self.counter = Counter()\n",
        "        self.vocab = None\n",
        "\n",
        "        self.optimal_sentence_length = None\n",
        "        self.should_include_text = should_include_text\n",
        "\n",
        "        if should_include_text:\n",
        "            self.columns = ['masked_sentence', self.MASKED_INDICES_COLUMN, 'sentence', self.TARGET_COLUMN,\n",
        "                            self.TOKEN_MASK_COLUMN,\n",
        "                            self.NSP_TARGET_COLUMN]\n",
        "        else:\n",
        "            self.columns = [self.MASKED_INDICES_COLUMN, self.TARGET_COLUMN, self.TOKEN_MASK_COLUMN,\n",
        "                            self.NSP_TARGET_COLUMN]\n",
        "        self.df = self.prepare_dataset()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        ...\n",
        "    \n",
        "    def prepare_dataset() -> pd.DataFrame:\n",
        "        ..."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "_jFEpyNFco5d",
        "outputId": "ada8d3de-9ba8-4cb3-8604-e43227041863"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-64f4fdcc245f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mIMDBBertDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Define Special tokens as attributes of class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mCLS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'[CLS]'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mPAD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'[PAD]'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mSEP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'[SEP]'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    733\u001b[0m             \u001b[0;31m# expected \"Union[Union[Union[ExtensionArray, ndarray],\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m             \u001b[0;31m# Index, Series], Sequence[Any]]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 735\u001b[0;31m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    736\u001b[0m             \u001b[0;31m# Argument 1 to \"ensure_index\" has incompatible type \"Collection[Any]\";\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m             \u001b[0;31m# expected \"Union[Union[Union[ExtensionArray, ndarray],\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mensure_index\u001b[0;34m(index_like, copy)\u001b[0m\n\u001b[1;32m   6334\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6336\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, data, dtype, copy, name, tupleize_cols, **kwargs)\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m             \u001b[0msubarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray_tuplesafe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"object\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    496\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, data, dtype, copy, name, tupleize_cols, **kwargs)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m                     \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_maybe_cast_data_without_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_maybe_cast_data_without_dtype\u001b[0;34m(subarr)\u001b[0m\n\u001b[1;32m   6415\u001b[0m         \u001b[0mconvert_period\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6416\u001b[0m         \u001b[0mconvert_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6417\u001b[0;31m         \u001b[0mdtype_if_all_nat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"datetime64[ns]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6418\u001b[0m     )\n\u001b[1;32m   6419\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"b\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.maybe_convert_objects\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Buffer has wrong number of dimensions (expected 1, got 3)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = []  \n",
        "nsp = []  \n",
        "sentence_lens = []\n",
        "\n",
        "# Split dataset on sentences\n",
        "for review in self.ds:\n",
        "    review_sentences = review.split('. ')\n",
        "    sentences += review_sentences\n",
        "    self._update_length(review_sentences, sentence_lens)\n",
        "self.optimal_sentence_length = self._find_optimal_sentence_length(sentence_lens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "id": "2zcHXPOwe-tH",
        "outputId": "b68561fd-5de6-4370-9bf2-15b546849819"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-17cf850a96e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Split dataset on sentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mreview\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mreview_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreview\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'. '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreview_sentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "from torch import nn\n",
        "import torch.nn.functional as f\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "class JointEmbedding(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, size):\n",
        "        super(JointEmbedding, self).__init__()\n",
        "\n",
        "        self.size = size\n",
        "\n",
        "        self.token_emb = nn.Embedding(vocab_size, size)\n",
        "        self.segment_emb = nn.Embedding(vocab_size, size)\n",
        "\n",
        "        self.norm = nn.LayerNorm(size)\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        sentence_size = input_tensor.size(-1)\n",
        "        pos_tensor = self.attention_position(self.size, input_tensor)\n",
        "\n",
        "        segment_tensor = torch.zeros_like(input_tensor).to(device)\n",
        "        segment_tensor[:, sentence_size // 2 + 1:] = 1\n",
        "\n",
        "        output = self.token_emb(input_tensor) + self.segment_emb(segment_tensor) + pos_tensor\n",
        "        return self.norm(output)\n",
        "\n",
        "    def attention_position(self, dim, input_tensor):\n",
        "        batch_size = input_tensor.size(0)\n",
        "        sentence_size = input_tensor.size(-1)\n",
        "\n",
        "        pos = torch.arange(sentence_size, dtype=torch.long).to(device)\n",
        "        d = torch.arange(dim, dtype=torch.long).to(device)\n",
        "        d = (2 * d / dim)\n",
        "\n",
        "        pos = pos.unsqueeze(1)\n",
        "        pos = pos / (1e4 ** d)\n",
        "\n",
        "        pos[:, ::2] = torch.sin(pos[:, ::2])\n",
        "        pos[:, 1::2] = torch.cos(pos[:, 1::2])\n",
        "\n",
        "        return pos.expand(batch_size, *pos.size())\n",
        "\n",
        "    def numeric_position(self, dim, input_tensor):\n",
        "        pos_tensor = torch.arange(dim, dtype=torch.long).to(device)\n",
        "        return pos_tensor.expand_as(input_tensor)\n",
        "\n",
        "\n",
        "class AttentionHead(nn.Module):\n",
        "\n",
        "    def __init__(self, dim_inp, dim_out):\n",
        "        super(AttentionHead, self).__init__()\n",
        "\n",
        "        self.dim_inp = dim_inp\n",
        "\n",
        "        self.q = nn.Linear(dim_inp, dim_out)\n",
        "        self.k = nn.Linear(dim_inp, dim_out)\n",
        "        self.v = nn.Linear(dim_inp, dim_out)\n",
        "\n",
        "    def forward(self, input_tensor: torch.Tensor, attention_mask: torch.Tensor = None):\n",
        "        query, key, value = self.q(input_tensor), self.k(input_tensor), self.v(input_tensor)\n",
        "\n",
        "        scale = query.size(1) ** 0.5\n",
        "        scores = torch.bmm(query, key.transpose(1, 2)) / scale\n",
        "\n",
        "        scores = scores.masked_fill_(attention_mask, -1e9)\n",
        "        attn = f.softmax(scores, dim=-1)\n",
        "        context = torch.bmm(attn, value)\n",
        "\n",
        "        return context\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, num_heads, dim_inp, dim_out):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "\n",
        "        self.heads = nn.ModuleList([\n",
        "            AttentionHead(dim_inp, dim_out) for _ in range(num_heads)\n",
        "        ])\n",
        "        self.linear = nn.Linear(dim_out * num_heads, dim_inp)\n",
        "        self.norm = nn.LayerNorm(dim_inp)\n",
        "\n",
        "    def forward(self, input_tensor: torch.Tensor, attention_mask: torch.Tensor):\n",
        "        s = [head(input_tensor, attention_mask) for head in self.heads]\n",
        "        scores = torch.cat(s, dim=-1)\n",
        "        scores = self.linear(scores)\n",
        "        return self.norm(scores)\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "\n",
        "    def __init__(self, dim_inp, dim_out, attention_heads=4, dropout=0.1):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.attention = MultiHeadAttention(attention_heads, dim_inp, dim_out)  # batch_size x sentence size x dim_inp\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(dim_inp, dim_out),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(dim_out, dim_inp),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "        self.norm = nn.LayerNorm(dim_inp)\n",
        "\n",
        "    def forward(self, input_tensor: torch.Tensor, attention_mask: torch.Tensor):\n",
        "        context = self.attention(input_tensor, attention_mask)\n",
        "        res = self.feed_forward(context)\n",
        "        return self.norm(res)\n",
        "\n",
        "\n",
        "class BERT(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, dim_inp, dim_out, attention_heads=4):\n",
        "        super(BERT, self).__init__()\n",
        "\n",
        "        self.embedding = JointEmbedding(vocab_size, dim_inp)\n",
        "        self.encoder = Encoder(dim_inp, dim_out, attention_heads)\n",
        "\n",
        "        self.token_prediction_layer = nn.Linear(dim_inp, vocab_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=-1)\n",
        "        self.classification_layer = nn.Linear(dim_inp, 2)\n",
        "\n",
        "    def forward(self, input_tensor: torch.Tensor, attention_mask: torch.Tensor):\n",
        "        embedded = self.embedding(input_tensor)\n",
        "        encoded = self.encoder(embedded, attention_mask)\n",
        "\n",
        "        token_predictions = self.token_prediction_layer(encoded)\n",
        "\n",
        "        first_word = encoded[:, 0, :]\n",
        "        return self.softmax(token_predictions), self.classification_layer(first_word)\n",
        "\n"
      ],
      "metadata": {
        "id": "Tb2LCuzUeUWo"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import typing\n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import Dataset\n",
        "from torchtext.vocab import vocab\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "class IMDBBertDataset(Dataset):\n",
        "    CLS = '[CLS]'\n",
        "    PAD = '[PAD]'\n",
        "    SEP = '[SEP]'\n",
        "    MASK = '[MASK]'\n",
        "    UNK = '[UNK]'\n",
        "\n",
        "    MASK_PERCENTAGE = 0.15\n",
        "\n",
        "    MASKED_INDICES_COLUMN = 'masked_indices'\n",
        "    TARGET_COLUMN = 'indices'\n",
        "    NSP_TARGET_COLUMN = 'is_next'\n",
        "    TOKEN_MASK_COLUMN = 'token_mask'\n",
        "\n",
        "    OPTIMAL_LENGTH_PERCENTILE = 70\n",
        "\n",
        "    def __init__(self, path, ds_from=None, ds_to=None, should_include_text=False):\n",
        "        self.ds: pd.Series = pd.read_csv(path)['review']\n",
        "\n",
        "        if ds_from is not None or ds_to is not None:\n",
        "            self.ds = self.ds[ds_from:ds_to]\n",
        "\n",
        "        self.tokenizer = get_tokenizer('basic_english')\n",
        "        self.counter = Counter()\n",
        "        self.vocab = None\n",
        "\n",
        "        self.optimal_sentence_length = None\n",
        "        self.should_include_text = should_include_text\n",
        "\n",
        "        if should_include_text:\n",
        "            self.columns = ['masked_sentence', self.MASKED_INDICES_COLUMN, 'sentence', self.TARGET_COLUMN,\n",
        "                            self.TOKEN_MASK_COLUMN,\n",
        "                            self.NSP_TARGET_COLUMN]\n",
        "        else:\n",
        "            self.columns = [self.MASKED_INDICES_COLUMN, self.TARGET_COLUMN, self.TOKEN_MASK_COLUMN,\n",
        "                            self.NSP_TARGET_COLUMN]\n",
        "\n",
        "        self.df = self.prepare_dataset()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.df.iloc[idx]\n",
        "\n",
        "        inp = torch.Tensor(item[self.MASKED_INDICES_COLUMN]).long()\n",
        "        token_mask = torch.Tensor(item[self.TOKEN_MASK_COLUMN]).bool()\n",
        "\n",
        "        mask_target = torch.Tensor(item[self.TARGET_COLUMN]).long()\n",
        "        mask_target = mask_target.masked_fill_(token_mask, 0)\n",
        "\n",
        "        attention_mask = (inp == self.vocab[self.PAD]).unsqueeze(0)\n",
        "\n",
        "        if item[self.NSP_TARGET_COLUMN] == 0:\n",
        "            t = [1, 0]\n",
        "        else:\n",
        "            t = [0, 1]\n",
        "\n",
        "        nsp_target = torch.Tensor(t)\n",
        "\n",
        "        return (\n",
        "            inp.to(device),\n",
        "            attention_mask.to(device),\n",
        "            token_mask.to(device),\n",
        "            mask_target.to(device),\n",
        "            nsp_target.to(device)\n",
        "        )\n",
        "\n",
        "    def prepare_dataset(self) -> pd.DataFrame:\n",
        "        sentences = []\n",
        "        nsp = []\n",
        "        sentence_lens = []\n",
        "\n",
        "        # Split dataset on sentences\n",
        "        for review in self.ds:\n",
        "            review_sentences = review.split('. ')\n",
        "            sentences += review_sentences\n",
        "            self._update_length(review_sentences, sentence_lens)\n",
        "        self.optimal_sentence_length = self._find_optimal_sentence_length(sentence_lens)\n",
        "\n",
        "        print(\"Create vocabulary\")\n",
        "        for sentence in tqdm(sentences):\n",
        "            s = self.tokenizer(sentence)\n",
        "            self.counter.update(s)\n",
        "\n",
        "        self._fill_vocab()\n",
        "\n",
        "        print(\"Preprocessing dataset\")\n",
        "        for review in tqdm(self.ds):\n",
        "            review_sentences = review.split('. ')\n",
        "            if len(review_sentences) > 1:\n",
        "                for i in range(len(review_sentences) - 1):\n",
        "                    # True NSP item\n",
        "                    first, second = self.tokenizer(review_sentences[i]), self.tokenizer(review_sentences[i + 1])\n",
        "                    nsp.append(self._create_item(first, second, 1))\n",
        "\n",
        "                    # False NSP item\n",
        "                    first, second = self._select_false_nsp_sentences(sentences)\n",
        "                    first, second = self.tokenizer(first), self.tokenizer(second)\n",
        "                    nsp.append(self._create_item(first, second, 0))\n",
        "        df = pd.DataFrame(nsp, columns=self.columns)\n",
        "        return df\n",
        "\n",
        "    def _update_length(self, sentences: typing.List[str], lengths: typing.List[int]):\n",
        "        for v in sentences:\n",
        "            l = len(v.split())\n",
        "            lengths.append(l)\n",
        "        return lengths\n",
        "\n",
        "    def _find_optimal_sentence_length(self, lengths: typing.List[int]):\n",
        "        arr = np.array(lengths)\n",
        "        return int(np.percentile(arr, self.OPTIMAL_LENGTH_PERCENTILE))\n",
        "\n",
        "    def _fill_vocab(self):\n",
        "        # specials= argument is only in 0.12.0 version\n",
        "        # specials=[self.CLS, self.PAD, self.MASK, self.SEP, self.UNK]\n",
        "        self.vocab = vocab(self.counter, min_freq=2)\n",
        "\n",
        "        # 0.11.0 uses this approach to insert specials\n",
        "        self.vocab.insert_token(self.CLS, 0)\n",
        "        self.vocab.insert_token(self.PAD, 1)\n",
        "        self.vocab.insert_token(self.MASK, 2)\n",
        "        self.vocab.insert_token(self.SEP, 3)\n",
        "        self.vocab.insert_token(self.UNK, 4)\n",
        "        self.vocab.set_default_index(4)\n",
        "\n",
        "    def _create_item(self, first: typing.List[str], second: typing.List[str], target: int = 1):\n",
        "        # Create masked sentence item\n",
        "        updated_first, first_mask = self._preprocess_sentence(first.copy())\n",
        "        updated_second, second_mask = self._preprocess_sentence(second.copy())\n",
        "\n",
        "        nsp_sentence = updated_first + [self.SEP] + updated_second\n",
        "        nsp_indices = self.vocab.lookup_indices(nsp_sentence)\n",
        "        inverse_token_mask = first_mask + [True] + second_mask\n",
        "\n",
        "        # Create sentence item without masking random words\n",
        "        first, _ = self._preprocess_sentence(first.copy(), should_mask=False)\n",
        "        second, _ = self._preprocess_sentence(second.copy(), should_mask=False)\n",
        "        original_nsp_sentence = first + [self.SEP] + second\n",
        "        original_nsp_indices = self.vocab.lookup_indices(original_nsp_sentence)\n",
        "\n",
        "        if self.should_include_text:\n",
        "            return (\n",
        "                nsp_sentence,\n",
        "                nsp_indices,\n",
        "                original_nsp_sentence,\n",
        "                original_nsp_indices,\n",
        "                inverse_token_mask,\n",
        "                target\n",
        "            )\n",
        "        else:\n",
        "            return (\n",
        "                nsp_indices,\n",
        "                original_nsp_indices,\n",
        "                inverse_token_mask,\n",
        "                target\n",
        "            )\n",
        "\n",
        "    def _select_false_nsp_sentences(self, sentences: typing.List[str]):\n",
        "        \"\"\"Select sentences to create false NSP item\n",
        "        Args:\n",
        "            sentences: list of all sentences\n",
        "        Returns:\n",
        "            tuple of two sentences. The second one NOT the next sentence\n",
        "        \"\"\"\n",
        "        sentences_len = len(sentences)\n",
        "        sentence_index = random.randint(0, sentences_len - 1)\n",
        "        next_sentence_index = random.randint(0, sentences_len - 1)\n",
        "\n",
        "        # To be sure that it's not real next sentence\n",
        "        while next_sentence_index == sentence_index + 1:\n",
        "            next_sentence_index = random.randint(0, sentences_len - 1)\n",
        "\n",
        "        return sentences[sentence_index], sentences[next_sentence_index]\n",
        "\n",
        "    def _preprocess_sentence(self, sentence: typing.List[str], should_mask: bool = True):\n",
        "        inverse_token_mask = None\n",
        "        if should_mask:\n",
        "            sentence, inverse_token_mask = self._mask_sentence(sentence)\n",
        "        sentence, inverse_token_mask = self._pad_sentence([self.CLS] + sentence, inverse_token_mask)\n",
        "\n",
        "        return sentence, inverse_token_mask\n",
        "\n",
        "    def _mask_sentence(self, sentence: typing.List[str]):\n",
        "        \"\"\"Replace MASK_PERCENTAGE (15%) of words with special [MASK] symbol\n",
        "        or with random word from vocabulary\n",
        "        Args:\n",
        "            sentence: sentence to process\n",
        "        Returns:\n",
        "            tuple of processed sentence and inverse token mask\n",
        "        \"\"\"\n",
        "        len_s = len(sentence)\n",
        "        inverse_token_mask = [True for _ in range(max(len_s, self.optimal_sentence_length))]\n",
        "\n",
        "        mask_amount = round(len_s * self.MASK_PERCENTAGE)\n",
        "        for _ in range(mask_amount):\n",
        "            i = random.randint(0, len_s - 1)\n",
        "\n",
        "            if random.random() < 0.8:\n",
        "                sentence[i] = self.MASK\n",
        "            else:\n",
        "                # All is below 5 is special token\n",
        "                # see self._insert_specials method\n",
        "                j = random.randint(5, len(self.vocab) - 1)\n",
        "                sentence[i] = self.vocab.lookup_token(j)\n",
        "            inverse_token_mask[i] = False\n",
        "        return sentence, inverse_token_mask\n",
        "\n",
        "    def _pad_sentence(self, sentence: typing.List[str], inverse_token_mask: typing.List[bool] = None):\n",
        "        len_s = len(sentence)\n",
        "\n",
        "        if len_s >= self.optimal_sentence_length:\n",
        "            s = sentence[:self.optimal_sentence_length]\n",
        "        else:\n",
        "            s = sentence + [self.PAD] * (self.optimal_sentence_length - len_s)\n",
        "\n",
        "        # inverse token mask should be padded as well\n",
        "        if inverse_token_mask:\n",
        "            len_m = len(inverse_token_mask)\n",
        "            if len_m >= self.optimal_sentence_length:\n",
        "                inverse_token_mask = inverse_token_mask[:self.optimal_sentence_length]\n",
        "            else:\n",
        "                inverse_token_mask = inverse_token_mask + [True] * (self.optimal_sentence_length - len_m)\n",
        "        return s, inverse_token_mask\n",
        "\n",
        "import os\n",
        "if __name__ == '__main__':\n",
        "    BASE_DIR = Path(os.path.abspath(numpy.__file__)).resolve().parent.parent\n",
        "\n",
        "    ds = IMDBBertDataset(BASE_DIR.joinpath('/content/IMDB Dataset.csv'), ds_from=0, ds_to=50000,\n",
        "                         should_include_text=True)\n",
        "    print(ds.df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8SuDl0dfKQv",
        "outputId": "6307b9b5-5d59-42f5-dc08-e6b74d4828db"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create vocabulary\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 491161/491161 [00:11<00:00, 41451.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50000/50000 [01:45<00:00, 476.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                          masked_sentence  \\\n",
            "0       [[CLS], [MASK], of, the, other, reviewers, gil...   \n",
            "1       [[CLS], he, has, no, real, love, or, loyalty, ...   \n",
            "2       [[CLS], they, are, right, ,, as, this, is, exa...   \n",
            "3       [[CLS], at, least, the, [MASK], porno, has, a,...   \n",
            "4       [[CLS], trust, me, ,, this, is, not, a, show, ...   \n",
            "...                                                   ...   \n",
            "882317  [[CLS], he, [MASK], !, i, saw, him, [PAD], [PA...   \n",
            "882318  [[CLS], no, one, expects, the, star, trek, mov...   \n",
            "882319  [[CLS], there, are, numerous, shots, of, peopl...   \n",
            "882320  [[CLS], unfortunately, ,, this, movie, had, a,...   \n",
            "882321  [[CLS], hell, ,, it, ', s, probably, the, best...   \n",
            "\n",
            "                                           masked_indices  \\\n",
            "0       [0, 2, 6, 7, 8, 9, 8333, 11, 12, 13, 14, 15, 1...   \n",
            "1       [0, 219, 10, 64, 384, 318, 61, 7501, 50, 881, ...   \n",
            "2       [0, 24, 25, 26, 27, 28, 29, 30, 31, 2, 2, 2, 3...   \n",
            "3       [0, 322, 673, 7, 2, 3994, 10, 56, 305, 1, 1, 1...   \n",
            "4       [0, 54, 35, 27, 29, 30, 55, 56, 57, 58, 49567,...   \n",
            "...                                                   ...   \n",
            "882317  [0, 219, 2, 224, 124, 148, 881, 1, 1, 1, 1, 1,...   \n",
            "882318  [0, 64, 5, 8571, 7, 756, 5005, 574, 67, 22, 10...   \n",
            "882319  [0, 354, 25, 1409, 399, 6, 418, 12166, 1362, 7...   \n",
            "882320  [0, 1381, 27, 29, 364, 537, 56, 10578, 27, 2, ...   \n",
            "882321  [0, 682, 27, 73, 20, 242, 491, 7, 462, 430, 37...   \n",
            "\n",
            "                                                 sentence  \\\n",
            "0       [[CLS], one, of, the, other, reviewers, has, m...   \n",
            "1       [[CLS], he, has, no, real, love, or, loyalty, ...   \n",
            "2       [[CLS], they, are, right, ,, as, this, is, exa...   \n",
            "3       [[CLS], at, least, the, old, porno, has, a, po...   \n",
            "4       [[CLS], trust, me, ,, this, is, not, a, show, ...   \n",
            "...                                                   ...   \n",
            "882317  [[CLS], he, was, !, i, saw, him, [PAD], [PAD],...   \n",
            "882318  [[CLS], no, one, expects, the, star, trek, mov...   \n",
            "882319  [[CLS], there, are, numerous, shots, of, peopl...   \n",
            "882320  [[CLS], unfortunately, ,, this, movie, had, a,...   \n",
            "882321  [[CLS], hell, ,, it, ', s, probably, the, best...   \n",
            "\n",
            "                                                  indices  \\\n",
            "0       [0, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,...   \n",
            "1       [0, 219, 10, 64, 384, 318, 61, 7501, 50, 881, ...   \n",
            "2       [0, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34...   \n",
            "3       [0, 322, 673, 7, 503, 3994, 10, 56, 305, 1, 1,...   \n",
            "4       [0, 54, 35, 27, 29, 30, 55, 56, 57, 58, 7, 59,...   \n",
            "...                                                   ...   \n",
            "882317  [0, 219, 41, 224, 124, 148, 881, 1, 1, 1, 1, 1...   \n",
            "882318  [0, 64, 5, 8571, 7, 756, 5005, 574, 67, 22, 10...   \n",
            "882319  [0, 354, 25, 1409, 399, 6, 418, 12166, 1362, 7...   \n",
            "882320  [0, 1381, 27, 29, 364, 537, 56, 10578, 27, 262...   \n",
            "882321  [0, 682, 27, 73, 20, 242, 491, 7, 462, 430, 37...   \n",
            "\n",
            "                                               token_mask  is_next  \n",
            "0       [False, True, True, True, True, False, True, T...        1  \n",
            "1       [True, True, True, True, True, True, True, Tru...        0  \n",
            "2       [True, True, True, True, True, True, True, Tru...        1  \n",
            "3       [True, True, True, False, True, True, True, Tr...        0  \n",
            "4       [True, True, True, True, True, True, True, Tru...        1  \n",
            "...                                                   ...      ...  \n",
            "882317  [True, False, True, True, True, True, True, Tr...        0  \n",
            "882318  [True, True, True, True, True, True, True, Tru...        1  \n",
            "882319  [True, True, True, True, True, True, True, Tru...        0  \n",
            "882320  [True, True, True, True, True, True, True, Tru...        1  \n",
            "882321  [True, True, True, True, True, True, True, Tru...        0  \n",
            "\n",
            "[882322 rows x 6 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "print(numpy.__file__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QtULaKyofTpR",
        "outputId": "5316b665-7dbf-4c16-f701-550286258127"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/lib/python3.7/random.py\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/__init__.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "# from bert.dataset import IMDBBertDataset\n",
        "# from bert.model import BERT\n",
        "\n",
        "IMDBBertDataset()\n",
        "BERT()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "def percentage(batch_size: int, max_index: int, current_index: int):\n",
        "    \"\"\"Calculate epoch progress percentage\n",
        "\n",
        "    Args:\n",
        "        batch_size: batch size\n",
        "        max_index: max index in epoch\n",
        "        current_index: current index\n",
        "\n",
        "    Returns:\n",
        "        Passed percentage of dataset\n",
        "    \"\"\"\n",
        "    batched_max = max_index // batch_size\n",
        "    return round(current_index / batched_max * 100, 2)\n",
        "\n",
        "\n",
        "def nsp_accuracy(result: torch.Tensor, target: torch.Tensor):\n",
        "    \"\"\"Calculate NSP accuracy between two tensors\n",
        "\n",
        "    Args:\n",
        "        result: result calculated by model\n",
        "        target: real target\n",
        "\n",
        "    Returns:\n",
        "        NSP accuracy\n",
        "    \"\"\"\n",
        "    s = (result.argmax(1) == target.argmax(1)).sum()\n",
        "    return round(float(s / result.size(0)), 2)\n",
        "\n",
        "\n",
        "def token_accuracy(result: torch.Tensor, target: torch.Tensor, inverse_token_mask: torch.Tensor):\n",
        "    \"\"\"Calculate MLM accuracy between ONLY masked words\n",
        "\n",
        "    Args:\n",
        "        result: result calculated by model\n",
        "        target: real target\n",
        "        inverse_token_mask: well-known inverse token mask\n",
        "\n",
        "    Returns:\n",
        "        MLM accuracy\n",
        "    \"\"\"\n",
        "    r = result.argmax(-1).masked_select(~inverse_token_mask)\n",
        "    t = target.masked_select(~inverse_token_mask)\n",
        "    s = (r == t).sum()\n",
        "    return round(float(s / (result.size(0) * result.size(1))), 2)\n",
        "\n",
        "\n",
        "class BertTrainer:\n",
        "\n",
        "    def __init__(self,\n",
        "                 model: BERT,\n",
        "                 dataset: IMDBBertDataset,\n",
        "                 log_dir: Path,\n",
        "                 checkpoint_dir: Path = None,\n",
        "                 print_progress_every: int = 10,\n",
        "                 print_accuracy_every: int = 50,\n",
        "                 batch_size: int = 24,\n",
        "                 learning_rate: float = 0.005,\n",
        "                 epochs: int = 5,\n",
        "                 ):\n",
        "        self.model = model\n",
        "        self.dataset = dataset\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        self.epochs = epochs\n",
        "        self.current_epoch = 0\n",
        "\n",
        "        self.loader = DataLoader(self.dataset, batch_size=self.batch_size, shuffle=True)\n",
        "\n",
        "        self.writer = SummaryWriter(str(log_dir))\n",
        "        self.checkpoint_dir = checkpoint_dir\n",
        "\n",
        "        self.criterion = nn.BCEWithLogitsLoss().to(device)\n",
        "        self.ml_criterion = nn.NLLLoss(ignore_index=0).to(device)\n",
        "        self.optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.015)\n",
        "\n",
        "        self._splitter_size = 35\n",
        "\n",
        "        self._ds_len = len(self.dataset)\n",
        "        self._batched_len = self._ds_len // self.batch_size\n",
        "\n",
        "        self._print_every = print_progress_every\n",
        "        self._accuracy_every = print_accuracy_every\n",
        "\n",
        "    def print_summary(self):\n",
        "        ds_len = len(self.dataset)\n",
        "\n",
        "        print(\"Model Summary\\n\")\n",
        "        print('=' * self._splitter_size)\n",
        "        print(f\"Device: {device}\")\n",
        "        print(f\"Training dataset len: {ds_len}\")\n",
        "        print(f\"Max / Optimal sentence len: {self.dataset.optimal_sentence_length}\")\n",
        "        print(f\"Vocab size: {len(self.dataset.vocab)}\")\n",
        "        print(f\"Batch size: {self.batch_size}\")\n",
        "        print(f\"Batched dataset len: {self._batched_len}\")\n",
        "        print('=' * self._splitter_size)\n",
        "        print()\n",
        "\n",
        "    def __call__(self):\n",
        "        for self.current_epoch in range(self.current_epoch, self.epochs):\n",
        "            loss = self.train(self.current_epoch)\n",
        "            self.save_checkpoint(self.current_epoch, step=-1, loss=loss)\n",
        "\n",
        "    def train(self, epoch: int):\n",
        "        print(f\"Begin epoch {epoch}\")\n",
        "\n",
        "        prev = time.time()\n",
        "        average_nsp_loss = 0\n",
        "        average_mlm_loss = 0\n",
        "        for i, value in enumerate(self.loader):\n",
        "            index = i + 1\n",
        "            inp, mask, inverse_token_mask, token_target, nsp_target = value\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            token, nsp = self.model(inp, mask)\n",
        "\n",
        "            tm = inverse_token_mask.unsqueeze(-1).expand_as(token)\n",
        "            token = token.masked_fill(tm, 0)\n",
        "\n",
        "            loss_token = self.ml_criterion(token.transpose(1, 2), token_target)  # 1D tensor as target is required\n",
        "            loss_nsp = self.criterion(nsp, nsp_target)\n",
        "\n",
        "            loss = loss_token + loss_nsp\n",
        "            average_nsp_loss += loss_nsp\n",
        "            average_mlm_loss += loss_token\n",
        "\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            if index % self._print_every == 0:\n",
        "                elapsed = time.gmtime(time.time() - prev)\n",
        "                s = self.training_summary(elapsed, index, average_nsp_loss, average_mlm_loss)\n",
        "\n",
        "                if index % self._accuracy_every == 0:\n",
        "                    s += self.accuracy_summary(index, token, nsp, token_target, nsp_target)\n",
        "\n",
        "                print(s)\n",
        "\n",
        "                average_nsp_loss = 0\n",
        "                average_mlm_loss = 0\n",
        "        return loss\n",
        "\n",
        "    def training_summary(self, elapsed, index, average_nsp_loss, average_mlm_loss):\n",
        "        passed = percentage(self.batch_size, self._ds_len, index)\n",
        "        global_step = self.current_epoch * len(self.loader) + index\n",
        "\n",
        "        print_nsp_loss = average_nsp_loss / self._print_every\n",
        "        print_mlm_loss = average_mlm_loss / self._print_every\n",
        "\n",
        "        s = f\"{time.strftime('%H:%M:%S', elapsed)}\"\n",
        "        s += f\" | Epoch {self.current_epoch + 1} | {index} / {self._batched_len} ({passed}%) | \" \\\n",
        "             f\"NSP loss {print_nsp_loss:6.2f} | MLM loss {print_mlm_loss:6.2f}\"\n",
        "\n",
        "        self.writer.add_scalar(\"NSP loss\", print_nsp_loss, global_step=global_step)\n",
        "        self.writer.add_scalar(\"MLM loss\", print_mlm_loss, global_step=global_step)\n",
        "        return s\n",
        "\n",
        "    def accuracy_summary(self, index, token, nsp, token_target, nsp_target, inverse_token_mask):\n",
        "        global_step = self.current_epoch * len(self.loader) + index\n",
        "        nsp_acc = nsp_accuracy(nsp, nsp_target)\n",
        "        token_acc = token_accuracy(token, token_target, inverse_token_mask)\n",
        "\n",
        "        self.writer.add_scalar(\"NSP train accuracy\", nsp_acc, global_step=global_step)\n",
        "        self.writer.add_scalar(\"Token train accuracy\", token_acc, global_step=global_step)\n",
        "\n",
        "        return f\" | NSP accuracy {nsp_acc} | Token accuracy {token_acc}\"\n",
        "\n",
        "    def save_checkpoint(self, epoch, step, loss):\n",
        "        if not self.checkpoint_dir:\n",
        "            return\n",
        "\n",
        "        prev = time.time()\n",
        "        name = f\"bert_epoch{epoch}_step{step}_{datetime.utcnow().timestamp():.0f}.pt\"\n",
        "\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': self.model.state_dict(),\n",
        "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "            'loss': loss,\n",
        "        }, self.checkpoint_dir.joinpath(name))\n",
        "\n",
        "        print()\n",
        "        print('=' * self._splitter_size)\n",
        "        print(f\"Model saved as '{name}' for {time.time() - prev:.2f}s\")\n",
        "        print('=' * self._splitter_size)\n",
        "        print()\n",
        "\n",
        "    def load_checkpoint(self, path: Path):\n",
        "        print('=' * self._splitter_size)\n",
        "        print(f\"Restoring model {path}\")\n",
        "        checkpoint = torch.load(path)\n",
        "        self.current_epoch = checkpoint['epoch']\n",
        "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        print(\"Model is restored.\")\n",
        "        print('=' * self._splitter_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "id": "eemZYSwlhV2c",
        "outputId": "7350213b-0340-4a04-adce-c31d922b1d8b"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-11ca71c42f1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# from bert.model import BERT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mIMDBBertDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mBERT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'path'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sbJrEzYniERA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}